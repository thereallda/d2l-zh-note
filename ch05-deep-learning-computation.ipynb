{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "060ec4ae",
   "metadata": {},
   "source": [
    "# Chapter05 Deep Learning Computation\n",
    "\n",
    "## Layers and Blocks\n",
    "\n",
    "在神经网络中，层（1）接受一组输入， （2）生成相应的输出， （3）由一组可调整参数描述。\n",
    "\n",
    "在层之上，模型之下，还有块（block）的概念\n",
    "\n",
    "> A block could describe a single layer, a component consisting of multiple layers, or the entire model itself! One benefit of working with the block abstraction is that they can be combined into larger artifacts, often recursively.\n",
    "\n",
    "![](https://zh.d2l.ai/_images/blocks.svg)\n",
    "\n",
    "以下的模型包含一个256个单元和ReLU激活函数的全连接隐藏层，然后是一个具有10个单元的全连接输出层。从输入和输出的角度而言，该模型接受20个输入，随后输出10个值。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3be1b44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0159,  0.1375, -0.0408,  0.0653,  0.0076, -0.1578,  0.0562,  0.1444,\n",
       "          0.1755, -0.1096],\n",
       "        [-0.0642,  0.1484, -0.0824,  0.0689, -0.0527, -0.2860,  0.0616,  0.1039,\n",
       "          0.1616, -0.1140]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)  # 生成2x20的随机变量\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9b807f",
   "metadata": {},
   "source": [
    "### A Custom Block\n",
    "\n",
    "The basic functionality that each block must provide:\n",
    "\n",
    "1. Ingest input data as arguments to its forward propagation function.\n",
    "\n",
    "2. Generate an output by having the forward propagation function return a value. Note that the output may have a different shape from the input. For example, the first fully-connected layer in our model above ingests an input of dimension 20 but returns an output of dimension 256.\n",
    "\n",
    "3. Calculate the gradient of its output with respect to its input, which can be accessed via its backpropagation function. Typically this happens automatically.\n",
    "\n",
    "4. Store and provide access to those parameters necessary to execute the forward propagation computation.\n",
    "\n",
    "5. Initialize model parameters as needed.\n",
    "\n",
    "以下构建一个块，对应了一个MLP，包含一个具有256个隐藏单元的隐藏层和一个10维的输出层。换言之，是对上面使用nn.Sequential类实现的MLP的一个复刻。\n",
    "\n",
    "输入X变量，返回该模型的输出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4631a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # 用模型参数声明层。这里，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        # 调用MLP的父类Module的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)  # 隐藏层存储在`hidden`变量\n",
    "        self.out = nn.Linear(256, 10)  # 输出层存储在`out`变量\n",
    "\n",
    "    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n",
    "    def forward(self, X):\n",
    "        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a434f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1810,  0.0155,  0.0203,  0.0334, -0.0949, -0.0509,  0.1669, -0.0060,\n",
       "          0.0367, -0.0119],\n",
       "        [-0.3329, -0.0183, -0.0604, -0.0246, -0.1749, -0.1821,  0.0909, -0.0308,\n",
       "          0.0311,  0.0179]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7021a04",
   "metadata": {},
   "source": [
    "### The Sequential Block\n",
    "\n",
    "手动实现一个Sequential类。\n",
    "\n",
    "其中输入的每一个module都存储在 `_modules` 这一个有序字典中，类似于一个存储列表。\n",
    "\n",
    "随后，前向传播函数按顺序执行每个modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04903ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # Here, `module` is an instance of a `Module` subclass. We save it\n",
    "            # in the member variable `_modules` of the `Module` class, and its\n",
    "            # type is OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict guarantees that members will be traversed in the order\n",
    "        # they were added\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c6690",
   "metadata": {},
   "source": [
    "使用 MySequential 构建上述的MLP，包含一个输入、隐藏层，激活函数和输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6c6709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0155,  0.1048,  0.0261,  0.0893, -0.0437,  0.3351, -0.0884, -0.0573,\n",
       "         -0.0702,  0.1365],\n",
       "        [-0.0354, -0.0133,  0.0901,  0.1411,  0.0035,  0.2139, -0.0629, -0.0223,\n",
       "         -0.1743,  0.2914]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a8947",
   "metadata": {},
   "source": [
    "### Exeecuting Code in the Forward Propagation Function\n",
    "\n",
    "除了利用块简单组合各个神经网络的组件之外，我们还可以在块中进行各种数学运算和控制流操作。\n",
    "\n",
    "以下实现一个FixedHiddenMLP类，通过在前向传播函数中的计算，输出一个常量\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "945927ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 构建一个不计算梯度的随机权重参数。因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96c5e9e",
   "metadata": {},
   "source": [
    "此处，模型做了一些不寻常的事情： 它运行了一个while循环，在范数大于的条件下， 将输出向量除以2，直到它满足条件为止。 最后，模型返回了X中所有项的和."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c61a51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1973, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0dd9a4",
   "metadata": {},
   "source": [
    "我们还可以创建混合嵌套块。\n",
    "\n",
    "这里注意的是，nn.Sequential接受module的输入，而NestMLP()和FixedHiddenMLP()都是Module的子类。因此，可以作为nn.Sequential的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d0fd03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2665, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e929973e",
   "metadata": {},
   "source": [
    "## 参数管理\n",
    "\n",
    "参数管理相关的代码\n",
    "\n",
    "对于一个单隐藏层的MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4521957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1879],\n",
       "        [-0.1612]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4)) # 生成2x4的随机矩阵\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87d137cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395717f7",
   "metadata": {},
   "source": [
    "Sequential类可以看作一个列表，包含了3个元素。\n",
    "\n",
    "### 参数访问\n",
    "\n",
    "通过 `net[2].state_dict()` 可以访问输出层的参数，包括权重和偏置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29c67415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.0985,  0.0370, -0.2010,  0.2683,  0.2943,  0.0443, -0.1458, -0.3514]])), ('bias', tensor([0.0249]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f46649f",
   "metadata": {},
   "source": [
    "- 访问特定参数\n",
    "\n",
    "通过 `.variable` 的方法可以访问特定参数。当前模型包含 `net` 和 `weight` 两个变量。 要注意 `net[2].bias.data` 才能访问到bias的数值，因为 `net[2].bias` 还包含了 grad 梯度。 \n",
    "\n",
    "由于现在还没进行后向计算，所以梯度未None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7826dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.0249], requires_grad=True)\n",
      "tensor([0.0249])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3bd9885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0985,  0.0370, -0.2010,  0.2683,  0.2943,  0.0443, -0.1458, -0.3514]],\n",
      "       requires_grad=True)\n",
      "tensor([[-0.0985,  0.0370, -0.2010,  0.2683,  0.2943,  0.0443, -0.1458, -0.3514]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(net[2].weight)\n",
    "print(net[2].weight.data)\n",
    "print(net[2].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b3b04",
   "metadata": {},
   "source": [
    "- 访问所有参数\n",
    "\n",
    "通过递归的方式访问每个模块的参数\n",
    "\n",
    "`net[1]` 是激活函数 ReLU() 并不包含参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df92fd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894698ca",
   "metadata": {},
   "source": [
    "对每层都有唯一的参数命名，如输出层的bias为 `2.bias`。 所以，我们可以通过参数的名字访问该参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ebcc7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0249])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9a13f7",
   "metadata": {},
   "source": [
    "### 从嵌套块中访问参数\n",
    "\n",
    "我们可以看看当多个块相互嵌套，需要如何访问参数\n",
    "\n",
    "首先定义块\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46b258f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2003],\n",
       "        [-0.2003]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())  \n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add_module(f'block {i}', block1()) # `f'block {i}'` 作用相当于传入block id\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e87d1527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2576640",
   "metadata": {},
   "source": [
    "因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。 下面，我们访问第一个主要的块中、第二个子块的第一层的偏置项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3336cd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0802,  0.4245, -0.2623,  0.1656, -0.0973,  0.4199, -0.0201,  0.2378])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569881c1",
   "metadata": {},
   "source": [
    "### 参数初始化\n",
    "\n",
    "- 内置初始化\n",
    "\n",
    "让我们首先调用内置的初始化器。 下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量， 且将偏置参数设置为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52669390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0155, -0.0071,  0.0015, -0.0058]), tensor(0.))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal) # apply递归整个网络中的参数执行 `init_normal`\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe02463",
   "metadata": {},
   "source": [
    "或者都初始化权重为常数1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca4aee16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d976ef8",
   "metadata": {},
   "source": [
    "或者对不同的块执行不同的初始化\n",
    "\n",
    "例如，下面我们使用Xavier初始化方法初始化第一个神经网络层， 然后将第三个神经网络层初始化为常量值42。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "203cb8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7008, -0.1801, -0.1880,  0.5484])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0]) # 只打印第一个元素\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf9a4ca",
   "metadata": {},
   "source": [
    "- 自定义初始化\n",
    "\n",
    "下面自定义一个初始化函数。首先，它会打印初始化哪个层的哪个参数的信息。随后，随机从均一分布中抽取初始权重值。然后，保留大于等于5的权重值，将剩余的值设为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ce5268c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.3258,  0.0000, -9.9274,  5.0150],\n",
       "        [-0.0000,  9.1049, -9.4709, -9.4527]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                        for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a41bb",
   "metadata": {},
   "source": [
    "此外，通过访问、赋值的方式，我们还可以直接设置参数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cefd35e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000,  1.0000, -8.9274,  6.0150])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b8bd79",
   "metadata": {},
   "source": [
    "### 参数绑定\n",
    "\n",
    "有时我们希望在多个层间共享参数： 我们可以定义一个稠密层 （dense layer），然后使用它的参数来设置另一个层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec4b164d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 我们需要给共享层一个名称，以便可以引用它的参数\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "net(X)\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100 # 改变第二层的参数会同时改变第三层\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775bb8d",
   "metadata": {},
   "source": [
    "## 自定义层\n",
    "\n",
    "### 不带参数的层\n",
    "\n",
    "首先，我们构造一个没有任何参数的自定义层。下面的CenteredLayer类要从其输入中减去均值。 要构建它，我们只需继承基础层类（init）并实现前向传播功能（forward）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e41ab31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a0e2558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c315637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.8626e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())\n",
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean() # 均值应该是0，但由于存储精度，会返回很小的非零值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a1aa1",
   "metadata": {},
   "source": [
    "### 带参数的层\n",
    "\n",
    "现在，让我们实现自定义版本的全连接层。 回想一下，该层需要两个参数，一个用于表示权重，另一个用于表示偏置项。这里利用 nn.Parameter 类实现\n",
    "\n",
    "该层需要输入参数：in_units和units，分别表示输入数和输出数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06d7efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear) # 使用ReLU作为激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8215d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.7595,  0.1646, -0.2728],\n",
       "        [-1.2000,  0.2022, -0.2345],\n",
       "        [-1.4211,  0.1634, -0.2554],\n",
       "        [-0.6991,  0.5870,  1.7453],\n",
       "        [-1.4708, -1.7918,  0.8820]], requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个单线性层\n",
    "linear = MyLinear(5, 3)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d0e1ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.6572],\n",
       "        [0.0000, 0.0000, 1.5790]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输入一个2*5的矩阵，利用该线性层进行计算\n",
    "linear(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "49fa3a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.7428],\n",
       "        [5.2207]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用自定义层构建一个模型\n",
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
    "net(torch.rand(2, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c75d50",
   "metadata": {},
   "source": [
    "## 读写文件\n",
    "\n",
    "### 加载和保存张量\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b656750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 存储标量\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')\n",
    "x2 = torch.load('x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5320b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 存储列表\n",
    "y = torch.zeros(4)\n",
    "torch.save([x, y],'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a69c2d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 存储字典\n",
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f8d99",
   "metadata": {},
   "source": [
    "### 加载和保存模型参数\n",
    "\n",
    "对pytorch而言，如果我们想保存模型，我们只能通过保存模型参数和模型函数定义的方法实现。\n",
    "\n",
    "首先，生成一个MLP作为示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b5b8d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8cc8d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数到 \"mlp.params\"\n",
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e48d3949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP() # 如果脱离当前环境，我们需要重新定义 `MLP()`\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "clone.eval() # “起始模型训练”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de95bb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比较两个模型的输出\n",
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc4cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
